---
draft: false
---

import YouTube from '@site/src/components/YouTube';


# W11. Distribution 03
Utility allows us to think of scale-free laws of intelligence: what does it take for any system to be intelligent? Many agents can sense, act, control, and model (predict) the world, but we still would not think of them as intelligent. In a psychology terms, habituation and classical conditioning are memory-based agentic state changes that allow for sophisticated strategic actions, but not true learning. However, being able to update a reward model gives us the ability to follow chains of actions, even if they present immediate punishment. We compare habituation and decision-making in plant intelligence to our robot models of intelligence using Q-learning.

---
## Pre-readings and Videos
This week's question is about systems-level intelligence vs. individual intelligence. The video covers Q-learning for a single agent, and the readings cover plant vs. forest intelligence.

### Q-learning
<YouTube id="0iqz4tcKN58" />
This video describes Q-learning in some detail.

### Forest Intelligence
This reading covers [forest intelligence through Mycorrhizal fungal networks](https://link.springer.com/chapter/10.1007/978-3-319-75596-0_10).

### Plant learning
This reading covers [plant learning](https://link.springer.com/article/10.1007/s00442-013-2873-7).


### Plant intelligence
This reading argues for [plant intelligence](https://academic.oup.com/aob/article/125/1/11/5575979).

---
## Summary of the Day

- **Activity**. Q-learning for a 1D agent.

---
## Learning Goals
1. Be able to describe a scale-free definition of intelligence.
2. Apply scale-free definitions of intelligence to analyze agents and collectives.
3. Design a learning algorithm that takes into account future actions and rewards.