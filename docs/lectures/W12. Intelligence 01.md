---
draft: true
---

import YouTube from '@site/src/components/YouTube';


# W12. Intelligence 01
We'll explore how neural networks learn to recognize patterns by building functions from data. Starting from the challenge of digit recognition, we'll construct networks neuron by neuron, discovering why layers create hierarchical representations and how non-linearity enables networks to approximate any function. We'll train neural networks ourselves using TensorFlow Playground, where you'll see how gradient descent navigates high-dimensional parameter spaces to minimize error. We'll connect these ideas to course themes of emergence and distributed cognition, examining how simple computations combine to produce complex behavior.

---
## Pre-readings and Videos
The videos today explain the basics of neural networks.

### Neural Networks
<YouTube id="aircAruvnKk" />
The beginning video of a series on deep learning and neural networks.

### Gradient Descent
<YouTube id="IHZwWFHWa-w" />
Gradient descent is one of the core mechanics of backpropogation.

---
## Summary of the Day

- **Activity**. [Tensor Flow Playground](https://playground.tensorflow.org/).

---
## Learning Goals
1. Understand neural networks as function approximators that learn mappings from training data
2. Explain why non-linear activation functions are essential for learning complex patterns
3. Describe how gradient descent searches parameter space to minimize error, and how backpropagation makes this computationally feasible